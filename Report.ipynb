{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif \"Trained Agent\"\n",
    "\n",
    "# Project 1: Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif \"Trained Agent\"\n",
    "[image2]: https://user-images.githubusercontent.com/10624937/42135622-e55fb586-7d12-11e8-8a54-3c31da15a90a.gif \"Soccer\"\n",
    "\n",
    "\n",
    "# Project 3: Collaboration and Competition\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this project, you will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.\n",
    "\n",
    "![Trained Agent][image1]\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MADDPG Agent\n",
    "MADDPG agent, [maddpg_agent.py](maddpg_agent.py), implements the MADDPG algorithm from [MADDPG paper](https://arxiv.org/abs/1706.02275). \n",
    "\n",
    "* MADDPG is a multi-agent version of DDPG. It is adapted from DDPG to resolve multi-agent collaboration and competition problems. Although the Tennis environment may be able to be solved by a simpler way, for the sake of learning, MADDPG is employed and tried.\n",
    "* To train multiple agents to work in a collaborative, completitive or mixed environment, the algorithm adopts the framework of centralized training with decentralized execution. During training, we feed critics in each agent with full observations, observations from all agents plus additional information if available, so that agents have opportunities to learn collaboration or completition. At the same time, actors in agents use only own observations but receiving suggestions from their critics for collaboration or completition information. That is the centralized training part. In testing or execution time, we only use actors in agents and there is no change required as they require the same own observation as training time.\n",
    "* Actor and critic network parameters used in the project. The actions are included in the second layer in the critic network.\n",
    "\n",
    "    Actor network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "            Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 24]              48\n",
    "                Linear-2                  [-1, 400]          10,000\n",
    "                Linear-3                  [-1, 300]         120,300\n",
    "                Linear-4                    [-1, 2]             602\n",
    "    ================================================================\n",
    "    Total params: 130,950\n",
    "    Trainable params: 130,950\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.01\n",
    "    Params size (MB): 0.50\n",
    "    Estimated Total Size (MB): 0.51\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "    \n",
    "    Critic network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "            Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 48]              96\n",
    "                Linear-2                  [-1, 400]          19,600\n",
    "                Linear-3                  [-1, 300]         121,500\n",
    "                Linear-4                    [-1, 1]             301\n",
    "    ================================================================\n",
    "    Total params: 141,497\n",
    "    Trainable params: 141,497\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.01\n",
    "    Params size (MB): 0.54\n",
    "    Estimated Total Size (MB): 0.55\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "* Except for the points mentioned above, the algorithm of MADDPG is similar to DDPG. For DDPG, you can find additional information in my write-up for the [Project 2](https://github.com/weicheng113/p2_continuous-control/blob/master/Report.ipynb)\n",
    "![Algorithm](./assets/algorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameters\n",
    "* Replay buffer size, BUFFER_SIZE, of **100,000**.\n",
    "* Minibatch size, BATCH_SIZE, of **256**.\n",
    "* Discount factor, GAMMA, of **0.99**.\n",
    "* Soft update of target parameters, TAU, of **0.001**.\n",
    "* Actor learning rate of **0.0001** and critic learning rate of **0.001**.\n",
    "* Unmodified Ornstein-Uhlenbeck noise: mu of **0.0**, theta of **0.15** and sigma of **0.2**.\n",
    "* Initial noise scale: initial_noise_scale of **1.0** and noise reduction factor: noise_reduction of **0.99**. These two parameters control noise reduction in each step.\n",
    "* Episodes before training: episodes_before_train of **300**. The agent collects enough samples before starting to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The following is training result, which is resolved in 1260 episodes with average score of 0.51 over the last 100 episodes.\n",
    "\n",
    "![Result](./assets/result.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Challenge: Play Soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using MA-PPO approach to train the agents. Adopt similar training framework as MADDPG, centralized training and decentralized execution.\n",
    "* Take full states **4 * 336** as input to critics. \n",
    "* A modification was made to the reward structure for the training only as follows.\n",
    "$ reward = ownReward * (1.0 - teamSpirit) + teamMemberReward * teamSprint $. **teamSpirit = 0.4** at the moment.\n",
    "* [Source code](https://github.com/weicheng113/deep-reinforcement-learning/tree/master/soccer-twos-ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5000 episodes, we can see performance of the trained agents below.\n",
    "* trained agents vs random agents - [youtube video](https://youtu.be/P0BsQ1wL7vA)\n",
    "* trained agents vs trained agents - [youtube video](https://youtu.be/T_IKQryICCI)\n",
    "\n",
    "The one from Unity ML-Agents, which is said to use independent PPO agents - [youtube video](https://www.youtube.com/watch?v=Hg3nmYD3DjQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "In MA-PPO for Soccer Twos, I removed batch normalization layer because of an issue in training. I believe batch normalization will improve training efficiency according to my prior experience with other projects. I would like to resolve batch normalization issue and bring it back when I have time. I have not got enough time to train a adapted version of MADDPG for discrete actions yet. This is something I would like to try also. Apart from these, I also want to try Prioritized Experience Reply for training efficency and Generalized Advantage Estimation to reduce bias for critics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
